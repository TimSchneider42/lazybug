\section{Conclusion}

In our work we developed a neural network engine, capable of playing Bughouse chess.
To overcome many of the challenges Bughouse introduces over regular chess, we develop an adaptation of a MCTS variant proposed in prior work\cite{alphago, alphazero}.
We show that our engine is capable of winning against conventional state-of-the-art Bughouse engines like Sjeng~\cite{sjeng} and Sunsetter~\cite{sunsetter}.
\\\\
However, as discussed in \autoref{sec:evaluation}, a lot of the playing strength seems to come from the move proposals generated by the neural network policy.
This indicates that there is probably potential for improvement in the value function.
A reason for this could be the quality of the human generated data, which is likely to contain blunders and misplays, adding high variance to the result of each game given a position.
This problem could be tackled in the future by filtering the data more strictly or utilizing self play similar to \citet{alphago} to generate high quality games.
\\\\
Additionally, the means of communication with our engine are currently very limited.
This is due to the fact that we found learning communication from the data to be a highly non-trivial task.
One option to include a richer communication in LazyBug is to define a set of commands (e.g. asking for pieces or not to move) and reward fulfilling these tasks in the MCTS search.
However, this would introduce a huge set of new hyperparameters, since the correlation between the value of a position and the value of fulfilling a command needs to be defined in some way.
Thus, this solution would require a huge amount of hand-engineering and seems not very clean from a machine learning perspective.
Another option is to use reinforcement learning and learn to use communication, for example by playing with another engine with hardcoded commands.
Yet this would require large amounts of computation power and time, since reinforcement learning is usually not very data efficient and was thus not done in this work.